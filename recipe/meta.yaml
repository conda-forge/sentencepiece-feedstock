{% set version = "0.1.96" %}

package:
  name: sentencepiece
  version: {{ version }}

source:
  url: https://github.com/google/sentencepiece/archive/v{{ version }}.tar.gz
  sha256: 5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a

build:
  number: 0

requirements:
  build:
    - python                                 # [build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]
    - cmake
    - {{ compiler('cxx') }}
    - gperftools  # [unix]
    - make
    - pkg-config
  host:
    - pip
    - python
  run:
    - python

test:
  imports:
    - sentencepiece
  requires:
    - pip
    - pytest
  source_files:
    - python/test
    - data
  commands:
    - pip check
    - spm_export_vocab --help  # [linux]
    - spm_normalize --help     # [linux]
    # upstream test suite expects to be run from PKG_ROOT/python
    - cd python && pytest test

about:
  home: "https://github.com/google/sentencepiece/"
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  summary: Unsupervised text tokenizer for Neural Network-based text generation.
  description: |
    SentencePiece is an unsupervised text tokenizer and detokenizer mainly for
    Neural Network-based text generation systems where the vocabulary size is
    predetermined prior to the neural model training.

    SentencePiece implements subword units (e.g., byte-pair-encoding (BPE)
    [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram
    language model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the
    extension of direct training from raw sentences. SentencePiece allows us to
    make a purely end-to-end system that does not depend on language-specific
    pre/postprocessing.

extra:
  recipe-maintainers:
    - setu4993
    - rluria14
    - ndmaxar
    - oblute
